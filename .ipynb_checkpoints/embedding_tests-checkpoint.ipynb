{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# General:\n",
    "########################################\n",
    "\n",
    "import pandas as pd, numpy as np, time\n",
    "\n",
    "########################################\n",
    "# String cleaning:\n",
    "########################################\n",
    "import unidecode, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "########################################\n",
    "# Embedding:\n",
    "########################################\n",
    "\n",
    "from gensim.utils import tokenize as tk\n",
    "from gensim.models import Word2Vec as w2v, FastText as ft, Doc2Vec as d2v, phrases as bigram\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD, NMF # PCA will not work on sparse matricies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/abstracts_concat.csv', index_col = 'pubmed_paper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[~df['abstract'].isna(),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [list(tk(i, lowercase=True, deacc=True)) #  encoding='utf8'\n",
    "#           for i in df['abstract']]\n",
    "# df['corpus'] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## Clean text strings function\n",
    "########################################\n",
    "\n",
    "def remove_short_strings(X, max_characters = 2, lower_case_only = True):\n",
    "    \"\"\"\n",
    "    Remove 1-2 letter words in list\n",
    "    :param X: List of raw strings\n",
    "    :param max_characters: Maximum size of string to remove\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"    \n",
    "    if lower_case_only:\n",
    "        regex_string = r'\\b[a-z]{1,%s}\\b' % (max_characters)    \n",
    "    else:\n",
    "        regex_string = r'\\b\\w{1,%s}\\b' % (max_characters)\n",
    "    X = list(map(lambda x: re.sub(regex_string,' ', x), X)) # remove 1-2 letter words \n",
    "    return(X)\n",
    "\n",
    "def to_lower(X):\n",
    "    \"\"\"\n",
    "    Remove non-European characters whilst keeping accented european characters in list\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.lower(), X))\n",
    "    return(X)\n",
    "\n",
    "def to_latin(X):\n",
    "    \"\"\"\n",
    "    Remove non-European characters whilst keeping accented european characters in list\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.encode(\"latin1\", errors=\"ignore\").decode('latin1'), X))\n",
    "    return(X)\n",
    "\n",
    "def replace_accents(X):\n",
    "    \"\"\"\n",
    "    Replace accented characters with non-accented characters in list\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: unidecode.unidecode(x), X))\n",
    "    return(X)\n",
    "\n",
    "def remove_punctuation_regex(remove_tildas = True, remove_numerics = False):\n",
    "    \"\"\"\n",
    "    Write regex to replace all non-alphanumeric characters, replacing them with a space.\n",
    "    Option as to whether to remove tildas (~) or numerical values not.\n",
    "    :param remove_tildas: Boolean, whether to remove tildas or not\n",
    "    :param remove_numerics: Boolean, whether to remove numerics or not    \n",
    "    :return X: Regex to do this\n",
    "    \"\"\"    \n",
    "    if remove_tildas and remove_numerics:\n",
    "        regex_string = r'[^a-zA-Z\\s]' # replace all non-alphabet characters with a space\n",
    "    elif remove_tildas:\n",
    "        regex_string = r'[^\\w\\s]' # replace all non-alphanumeric characters with a space\n",
    "    elif remove_numerics:\n",
    "        regex_string = r'[^a-zA-Z\\s\\~]' # replace all non-alphabet characters except tildas with a space        \n",
    "    else:\n",
    "        regex_string = r'[^\\w\\s\\~]' # replace all non-alphanumeric characters except tildas with a space\n",
    "    return(regex_string)\n",
    "\n",
    "def remove_punctuation(X, regex_string):\n",
    "    \"\"\"\n",
    "    Replace all non-alphanumeric characters in a list of strings, replacing them with a space. \n",
    "    Option as to whether to remove tildas (~) or not.\n",
    "    :param X: List of raw strings\n",
    "    :param remove_tildas: Boolean, whether to remove tildas or not\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"    \n",
    "    X = list(map(lambda x: re.sub(regex_string,' ', x), X)) \n",
    "    return(X)\n",
    "\n",
    "def tokenise(X, delimeter = None):\n",
    "    \"\"\"\n",
    "    Returns list of lists of strings split by the delimeter\n",
    "    :param X: List of strings\n",
    "    :param delimeter: Delimeter to split by\n",
    "    :return X: List of list of strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.split(sep = delimeter), X))\n",
    "    return(X)\n",
    "\n",
    "def remove_stopwords(X, stop_words = stopwords.words('english')):\n",
    "    \"\"\"\n",
    "    Returns list of lists of strings split by the delimeter\n",
    "    :param X: List of strings\n",
    "    :param delimeter: Delimeter to split by\n",
    "    :return X: List of list of strings\n",
    "    \"\"\"\n",
    "    # X = list(map(lambda x: [w if w not in stop_words else None for w in x], X))\n",
    "    X = list(map(lambda x: [w for w in x if w not in stop_words ], X))\n",
    "    return(X)\n",
    "    # https://stackoverflow.com/questions/4260280/if-else-in-a-list-comprehension\n",
    "\n",
    "def stem_strings(X):\n",
    "    \"\"\"\n",
    "    Stems words (shorten algorithmically) them (as defined by SnowballStemmer)\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: SnowballStemmer(\"english\", ignore_stopwords=False).stem(x), X))\n",
    "    return(X)  \n",
    "\n",
    "def lemmatize_strings(X, pos = \"v\"):\n",
    "    \"\"\"\n",
    "    Lemmatize list of strings (as defined by WordNetLemmatizer)\n",
    "    :param X: List of raw strings\n",
    "    :param pos: Pos parameter to feed into WordNetLemmatizer().lemmatize function\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: WordNetLemmatizer().lemmatize(x, pos=pos), X))\n",
    "    return(X)  \n",
    "\n",
    "def clean_strings(X\n",
    "                  , remove_short_str_max_char = 2\n",
    "                  , to_lower_str = True\n",
    "                  , to_latin_str = True\n",
    "                  , replace_accents_str = True\n",
    "                  , regex_string =  r'[^a-zA-Z\\s-]' # remove_punctuation_regex(True, False)\n",
    "                  , tokenise_delimeter = None\n",
    "                  , stop_words = ''\n",
    "                  , stemming_str = False\n",
    "                  , lemma_str = False\n",
    "                  , lemma_pos = \"v\"\n",
    "                  , verbose = False):\n",
    "    \"\"\"\n",
    "    Combination of functions for a list of strings: see parameters\n",
    "    - Replaces non-alpha-numeric characters with whitespace\n",
    "    - Remove english stopwords from and strings and stems them (as defined by SnowballStemmer)\n",
    "    - Lemmatizes english strings (as defined by WordNetLemmatizer) \n",
    "    :param X: List of strings\n",
    "    :param remove_short_str: Numeric, size of small words to remove (if set to 0, no words are removed)\n",
    "    :param to_latin_str: Boolean, whether to remove non-European characters whilst keeping accented european characters from pandas column\n",
    "    :param replace_accents_str: Boolean, whether to replace accented characters with non-accented characters\n",
    "    :param regex_string: String, can add extra regex to find other characters to remove\n",
    "    :param tokenise_delimeter: String, determines how to split into tokens. Default = None splits by all whitespace\n",
    "    :param stop_words: List of stopwords to remove from the tokens\n",
    "    :param stemming_str: Boolean, whether to stem the words or not (do not use before translating) (as defined by SnowballStemmer)\n",
    "    :param lemma_str: Boolean, whether to lemmatize the words or not (do not use before translating) (as defined by WordNetLemmatizer)\n",
    "    :param lemma_pos: String, pos parameter to feed into WordNetLemmatizer().lemmatize function\n",
    "    :param verbose: whether to print when it finishes/comments\n",
    "    :return X: Dataframe of labelled data\n",
    "    \"\"\"\n",
    "    if remove_short_str_max_char > 0:\n",
    "        X = remove_short_strings(X, remove_short_str_max_char) # remove 1-2 letter words     \n",
    "    if to_lower_str: # remove chinese characters, keep accented european characters\n",
    "        X = to_lower(X)     \n",
    "    if to_latin_str: # remove chinese characters, keep accented european characters\n",
    "        X = to_latin(X) \n",
    "    if replace_accents_str: # replace accented characters with non-accented characters\n",
    "        X = replace_accents(X) \n",
    "    X = remove_punctuation(X, regex_string)\n",
    "    X = tokenise(X, tokenise_delimeter)\n",
    "    X = remove_stopwords(X, stop_words)\n",
    "    if stemming_str:\n",
    "        X = list(map(stem_strings, X)) # remove English stopwords from string (as defined by SnowballStemmer)\n",
    "    if lemma_str:\n",
    "        X = list(map(lambda x: lemmatize_strings(x, lemma_pos), X)) # remove English stopwords from string (as defined by SnowballStemmer)        \n",
    "    if verbose:\n",
    "        print(time.strftime('%d/%m/%Y %H:%M:%S') + ' Menu item strings cleaned')\n",
    "    return(X)\n",
    "\n",
    "# https://chrisalbon.com/machine_learning/preprocessing_text/remove_stop_words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/05/2019 12:51:24 Menu item strings cleaned\n"
     ]
    }
   ],
   "source": [
    "# CLEAN AND ABSTRACTS INTO STEMMED TOKENS\n",
    "df['clean'] = clean_strings(df.loc[:, 'abstract']\n",
    "                            , regex_string='[^a-zA-Z\\\\\\\\s]'\n",
    "                            , stop_words = stopwords.words('english')\n",
    "                            , verbose = True)\n",
    "df['stem'] = list(map(stem_strings, df['clean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE MAPPING BETWEEN STEMMED TOKENS AND CLEANED WORDS\n",
    "unique_words = list(set([item for sublist in df['clean'] for item in sublist]))\n",
    "stemmed_words = stem_strings(unique_words)\n",
    "stem_to_word = dict(zip(unique_words, stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF unigram word-level scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['stem_concat'] = list(map(lambda x: ' '.join(x), df['stem']))\n",
    "tf_idf = TfidfVectorizer(preprocessor=' '.join\n",
    "                         # , token_pattern = '[^ ]+'\n",
    "#                          , token_pattern = '[^a-zA-Z\\\\\\\\s]'\n",
    "                        ) # norm=None # IF WANT ACTUAL WORD COUNTS, RATHER THAN L2\n",
    "tf_idf_m = tf_idf.fit_transform(df['stem'])\n",
    "tf_m = tf_idf_m.multiply(1/tf_idf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab = pd.DataFrame(data = (tf_m != 0).sum(axis = 0).tolist()[0]\n",
    "                        , index = tf_idf.get_feature_names()\n",
    "                        , columns=['total_docs_inc'])\n",
    "\n",
    "df_vocab['total_tf'] = tf_m.sum(axis=0).tolist()[0]\n",
    "df_vocab['avg_nonzero_tf'] = tf_m.sum(axis=0).tolist()[0]/df_vocab['total_docs_inc']\n",
    "df_vocab['idf'] = tf_idf.idf_\n",
    "df_vocab['avg_nonzero_tfidf'] = tf_idf_m.sum(axis=0).tolist()[0]/df_vocab['total_docs_inc']\n",
    "# df_vocab['avg_tfidf'] = tfs.mean(axis=0).tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF tri-gram character-level scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['stem_concat'] = list(map(lambda x: ' '.join(x), df['stem']))\n",
    "unique_words = list(set([item for sublist in df.clean for item in sublist]))\n",
    "tf_idf_c = TfidfVectorizer(analyzer='char', ngram_range=(1,3)) # norm=None # IF WANT ACTUAL WORD COUNTS, RATHER THAN L2\n",
    "tf_idf_c_m = tf_idf_c.fit_transform(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_c_cosim_function(tf_idf_fit, tf_idf_matrix, vectorized_string = 'steril'):\n",
    "    vectorized_string = tf_idf_fit.transform([vectorized_string])\n",
    "    cos_sim_matrix = tf_idf_matrix.multiply(vectorized_string).sum(axis = 1)\n",
    "    X = {'return_string': unique_words[cos_sim_matrix.argmax()], 'return_cosine_similarity': cos_sim_matrix.max()}\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(any(\"taus-29\" == s for s in unique_words))\n",
    "# print(any(\"tau\" == s for s in unique_words))\n",
    "# tfidf_c_cosim_function(tf_idf_c,tf_idf_c_m,'taus-29')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-2-Vec Cosine-similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZE WORDS USING WORD-2-VEC\n",
    "# model = w2v(df['clean'], size=100, window=5, min_count=1, workers=4)\n",
    "model = w2v(df['stem'], size=100, window=5, min_count=1, workers=4)\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to return top X Word-2-Vec cosine similarities and TF-IDF unigram scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching = [s for s in df.abstract if \"conducta\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEFINE FUNCTION\n",
    "# def search_similar(word = 'practicable', number_matches = 10):\n",
    "#     df_sim = pd.DataFrame(columns = ['search_word', 'search_stem', 'return_stem', 'return_word', 'cosine_similarity', 'total_docs_inc', 'total_tf', 'avg_nonzero_tf', 'idf', 'avg_nonzero_tfidf'])\n",
    "#     word_to_search = stem_to_word[word]\n",
    "#     similarity = dict(model.wv.similar_by_vector(word_to_search, topn = number_matches))\n",
    "#     for i in range(number_matches):\n",
    "#         token_to_search = list(similarity.keys())[i]\n",
    "#         X = [list(stem_to_word.keys())[list(stem_to_word.values()).index(token_to_search)]]\n",
    "#         df_sim.loc[i, ['search_word', 'search_stem', 'return_stem', 'return_word', 'cosine_similarity']] = [word, word_to_search, token_to_search, X[0], list(similarity.values())[i]]\n",
    "#         df_sim.loc[i,list(df_vocab.columns)] = df_vocab.loc[df_vocab.index == token_to_search,:].values\n",
    "#     # # X = [list(stem_to_word.keys())[list(stem_to_word.values()).index(token_to_search)]]\n",
    "#     # # X\n",
    "#     return(df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_similar('tau', number_matches = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nonexact_similar(word = 'taus-29', number_matches = 10):\n",
    "    df_sim = pd.DataFrame(columns = ['search_word', 'best_match', 'best_match_tfidf_cosine_similarity', 'search_stem', 'return_stem', 'return_word', 'w2v_cosine_similarity', 'total_docs_inc', 'total_tf', 'avg_nonzero_tf', 'idf', 'avg_nonzero_tfidf'])\n",
    "    best_match = tfidf_c_cosim_function(tf_idf_c,tf_idf_c_m, word)\n",
    "    word_to_search = stem_to_word[best_match['return_string']]\n",
    "    similarity = dict(model.wv.similar_by_vector(word_to_search, topn = number_matches))\n",
    "    for i in range(number_matches):\n",
    "        token_to_search = list(similarity.keys())[i]\n",
    "        X = [list(stem_to_word.keys())[list(stem_to_word.values()).index(token_to_search)]]\n",
    "        df_sim.loc[i, ['search_word'\n",
    "                       , 'best_match', 'best_match_tfidf_cosine_similarity'\n",
    "                       , 'search_stem', 'return_stem', 'return_word'\n",
    "                       , 'w2v_cosine_similarity']] = \\\n",
    "        [word, best_match['return_string'], best_match['return_cosine_similarity'], word_to_search, token_to_search, X[0], list(similarity.values())[i]]\n",
    "        df_sim.loc[i,list(df_vocab.columns)] = df_vocab.loc[df_vocab.index == token_to_search,:].values\n",
    "    # # X = [list(stem_to_word.keys())[list(stem_to_word.values()).index(token_to_search)]]\n",
    "    # # X\n",
    "    return(df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_word</th>\n",
       "      <th>best_match</th>\n",
       "      <th>best_match_tfidf_cosine_similarity</th>\n",
       "      <th>search_stem</th>\n",
       "      <th>return_stem</th>\n",
       "      <th>return_word</th>\n",
       "      <th>w2v_cosine_similarity</th>\n",
       "      <th>total_docs_inc</th>\n",
       "      <th>total_tf</th>\n",
       "      <th>avg_nonzero_tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>avg_nonzero_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>salivari</td>\n",
       "      <td>salivary</td>\n",
       "      <td>0.761935</td>\n",
       "      <td>54</td>\n",
       "      <td>1.09744</td>\n",
       "      <td>0.020323</td>\n",
       "      <td>6.93133</td>\n",
       "      <td>0.140866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>corticosteron</td>\n",
       "      <td>corticosterone</td>\n",
       "      <td>0.730851</td>\n",
       "      <td>81</td>\n",
       "      <td>1.28534</td>\n",
       "      <td>0.0158684</td>\n",
       "      <td>6.53194</td>\n",
       "      <td>0.103651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>progesteron</td>\n",
       "      <td>progesteron</td>\n",
       "      <td>0.712441</td>\n",
       "      <td>31</td>\n",
       "      <td>0.782391</td>\n",
       "      <td>0.0252384</td>\n",
       "      <td>7.47293</td>\n",
       "      <td>0.188605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>cort</td>\n",
       "      <td>cort</td>\n",
       "      <td>0.703391</td>\n",
       "      <td>25</td>\n",
       "      <td>0.934951</td>\n",
       "      <td>0.037398</td>\n",
       "      <td>7.68057</td>\n",
       "      <td>0.287238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>tsst</td>\n",
       "      <td>tsst</td>\n",
       "      <td>0.671725</td>\n",
       "      <td>10</td>\n",
       "      <td>0.305349</td>\n",
       "      <td>0.0305349</td>\n",
       "      <td>8.54077</td>\n",
       "      <td>0.260791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>diurnal</td>\n",
       "      <td>diurnal</td>\n",
       "      <td>0.658733</td>\n",
       "      <td>51</td>\n",
       "      <td>0.938129</td>\n",
       "      <td>0.0183947</td>\n",
       "      <td>6.98742</td>\n",
       "      <td>0.128531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>acth</td>\n",
       "      <td>acth</td>\n",
       "      <td>0.652056</td>\n",
       "      <td>13</td>\n",
       "      <td>0.347264</td>\n",
       "      <td>0.0267126</td>\n",
       "      <td>8.2996</td>\n",
       "      <td>0.221704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>prolactin</td>\n",
       "      <td>prolactin</td>\n",
       "      <td>0.650878</td>\n",
       "      <td>23</td>\n",
       "      <td>0.423214</td>\n",
       "      <td>0.0184006</td>\n",
       "      <td>7.76061</td>\n",
       "      <td>0.1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>testosteron</td>\n",
       "      <td>testosterone</td>\n",
       "      <td>0.646234</td>\n",
       "      <td>38</td>\n",
       "      <td>0.842454</td>\n",
       "      <td>0.0221699</td>\n",
       "      <td>7.2751</td>\n",
       "      <td>0.161288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>corisol</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>cortisol</td>\n",
       "      <td>crp</td>\n",
       "      <td>crp</td>\n",
       "      <td>0.638096</td>\n",
       "      <td>35</td>\n",
       "      <td>1.04294</td>\n",
       "      <td>0.0297984</td>\n",
       "      <td>7.35514</td>\n",
       "      <td>0.219172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  search_word best_match best_match_tfidf_cosine_similarity search_stem  \\\n",
       "0     corisol   cortisol                           0.674237    cortisol   \n",
       "1     corisol   cortisol                           0.674237    cortisol   \n",
       "2     corisol   cortisol                           0.674237    cortisol   \n",
       "3     corisol   cortisol                           0.674237    cortisol   \n",
       "4     corisol   cortisol                           0.674237    cortisol   \n",
       "5     corisol   cortisol                           0.674237    cortisol   \n",
       "6     corisol   cortisol                           0.674237    cortisol   \n",
       "7     corisol   cortisol                           0.674237    cortisol   \n",
       "8     corisol   cortisol                           0.674237    cortisol   \n",
       "9     corisol   cortisol                           0.674237    cortisol   \n",
       "\n",
       "     return_stem     return_word w2v_cosine_similarity total_docs_inc  \\\n",
       "0       salivari        salivary              0.761935             54   \n",
       "1  corticosteron  corticosterone              0.730851             81   \n",
       "2    progesteron     progesteron              0.712441             31   \n",
       "3           cort            cort              0.703391             25   \n",
       "4           tsst            tsst              0.671725             10   \n",
       "5        diurnal         diurnal              0.658733             51   \n",
       "6           acth            acth              0.652056             13   \n",
       "7      prolactin       prolactin              0.650878             23   \n",
       "8    testosteron    testosterone              0.646234             38   \n",
       "9            crp             crp              0.638096             35   \n",
       "\n",
       "   total_tf avg_nonzero_tf      idf avg_nonzero_tfidf  \n",
       "0   1.09744       0.020323  6.93133          0.140866  \n",
       "1   1.28534      0.0158684  6.53194          0.103651  \n",
       "2  0.782391      0.0252384  7.47293          0.188605  \n",
       "3  0.934951       0.037398  7.68057          0.287238  \n",
       "4  0.305349      0.0305349  8.54077          0.260791  \n",
       "5  0.938129      0.0183947  6.98742          0.128531  \n",
       "6  0.347264      0.0267126   8.2996          0.221704  \n",
       "7  0.423214      0.0184006  7.76061            0.1428  \n",
       "8  0.842454      0.0221699   7.2751          0.161288  \n",
       "9   1.04294      0.0297984  7.35514          0.219172  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_nonexact_similar('corisol', number_matches = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should create a custom class for tf-idf\n",
    "tfidf = TfidfVectorizer(analyzer='word'\n",
    "                        , lowercase=True\n",
    "                        , strip_accents='unicode'\n",
    "                        , ngram_range=(1,3)\n",
    "                        , norm = 'l2'\n",
    "                        , use_idf=True\n",
    "                        , smooth_idf = True)\n",
    "tfs = tfidf.fit_transform(corpus_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.test.utils import common_corpus, common_dictionary\n",
    "# from gensim.sklearn_api import TfIdfTransformer\n",
    "\n",
    "# model = TfIdfTransformer(dictionary=common_dictionary)\n",
    "# tfidf_corpus = model.fit_transform(common_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.phrases import Phrases\n",
    "# bigram = Phrases(common_texts)\n",
    "# common_texts = [bigram[line] for line in common_texts]\n",
    "# trigram = Phrases(common_texts)\n",
    "# common_texts = [trigram[line] for line in common_texts]\n",
    "\n",
    "# # trigram = Phrases(bigram.vocab.keys(), min_count=1, threshold=1)  # train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to tune hyperparameters for W2V model\n",
    "model = w2v(corpus, size=100, window=5, min_count=1, workers=4)\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# General:\n",
    "########################################\n",
    "\n",
    "import pandas as pd, numpy as np, os, sys, copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################\n",
    "# String cleaning:\n",
    "########################################\n",
    "import unidecode, re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "########################################\n",
    "# For the custom pipeline architecture:\n",
    "########################################\n",
    "import itertools\n",
    "# from itertools import chain, itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # to define class for use in pipeline\n",
    "\n",
    "##########\n",
    "# Tokenisation\n",
    "##########\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "\n",
    "##########\n",
    "# Dimensionality reduction\n",
    "##########\n",
    "from sklearn.decomposition import TruncatedSVD, NMF # PCA will not work on sparse matricies\n",
    "\n",
    "##########\n",
    "# Classification\n",
    "##########\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "##########\n",
    "# Pipeline\n",
    "##########\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "##########\n",
    "# Measuring performance\n",
    "##########\n",
    "from sklearn.metrics import average_precision_score, make_scorer, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parkinson', 0.7690507173538208),\n",
       " ('ad', 0.6871845126152039),\n",
       " ('neuropathology', 0.6836362481117249),\n",
       " ('huntington', 0.6725811958312988),\n",
       " ('prodromal', 0.6661573648452759),\n",
       " ('pathology', 0.6650171279907227),\n",
       " ('creutzfeldt', 0.6578435897827148),\n",
       " ('dementia', 0.6444272994995117),\n",
       " ('neuropathological', 0.6371122002601624),\n",
       " ('amyloidosis', 0.6302556395530701)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector('alzheimer', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class custom_tfidf(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self\n",
    "                 , X\n",
    "                 , string_delimeter = ' ~~ '\n",
    "                 , ngram_range= (1,2)\n",
    "                 , min_df = 1\n",
    "                 , max_df = 1.0\n",
    "                 , norm = 'l2'\n",
    "                 , use_idf = True\n",
    "                 , smooth_idf = True\n",
    "                 , sublinear_tf = True):\n",
    "        \"\"\"\n",
    "        Stems and create vocabulary based on delimeter; run text-frequency inverse-document-frequency (tf-idf) \n",
    "\n",
    "        :param X: array of strings\n",
    "        :param string_delimeter: array of strings\n",
    "        :param ngram_range: tuple of how many words to include in each tokenisation (e.g. (1,1) is unigram, (1,2) is bigram etc). See sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "        :param min_df: minimum # documents a word must be found in to include the word in the dictionary (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "        :param max_df: maximum # documents a word must be found in to include the word in the dictionary (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "        :param norm: array of rx_ids\n",
    "        :param use_idf: boolean, whether to use inverse-document-frequency in addition to just text-frequency  \n",
    "        :param smooth_idf: boolean, whether to add one to denominator in idf step to prevent div/0 errors (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "        :param sublinear_tf: boolean, whether to also log-transform the text-frequency step (see sklearn.feature_extraction.text.CountVectorizer for more details) \n",
    "        \"\"\"\n",
    "        self.train_X = X\n",
    "        self.string_delim = string_delimeter\n",
    "        self.ngram = ngram_range\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.norm = norm\n",
    "        self.use_idf = use_idf\n",
    "        self.smooth_idf = smooth_idf\n",
    "        self.sublinear_tf = sublinear_tf\n",
    "        \n",
    "    def fit(self, *_): # kwargs # fit()\n",
    "        \"\"\"\n",
    "        Create vocabulary dictionary (prevents bi+-gramming over seperate documents)\n",
    "        \"\"\"\n",
    "        split_strings = list(itertools.chain.from_iterable([re.split(self.string_delim, x) for x in self.train_X]))\n",
    "        stemmed = clean_strings(split_strings)\n",
    "        count_vec = CountVectorizer(strip_accents = 'unicode'\n",
    "                                    , analyzer = 'word'\n",
    "                                    , stop_words = 'english'\n",
    "                                    , lowercase = True\n",
    "                                    , min_df = self.min_df , max_df = self.max_df\n",
    "                                    , ngram_range = self.ngram)\n",
    "        vocab = count_vec.fit(stemmed)\n",
    "        self.vocab = vocab.vocabulary_\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, X): # kwargs # transform\n",
    "        \"\"\"\n",
    "        Run tf-idf using vocab\n",
    "        \"\"\"        \n",
    "        stemmed = clean_strings(X)\n",
    "        tfidf_vec = TfidfVectorizer(strip_accents = 'unicode'\n",
    "                                    , analyzer = 'word'\n",
    "                                    , stop_words = 'english'\n",
    "                                    , lowercase = True\n",
    "                                    , min_df = self.min_df , max_df = self.max_df\n",
    "                                    , ngram_range = self.ngram\n",
    "                                    , vocabulary = self.vocab\n",
    "                                    , norm = self.norm\n",
    "                                    , use_idf = self.use_idf\n",
    "                                    , smooth_idf =  self.smooth_idf\n",
    "                                    , sublinear_tf = self.sublinear_tf)\n",
    "        vectorized_matrix = tfidf_vec.fit_transform(stemmed)\n",
    "        return(vectorized_matrix)\n",
    "    \n",
    "def tokenisation(train_X, validation_X\n",
    "                 , string_delimeter = ' ~~ '\n",
    "                 , ngram_range= (1,2)\n",
    "                 , min_df = 1\n",
    "                 , max_df = 1.0\n",
    "                 , norm = 'l2'\n",
    "                 , use_idf = True\n",
    "                 , smooth_idf = True\n",
    "                 , sublinear_tf = True):\n",
    "    ct = custom_tfidf(train_X)\n",
    "    a = ct.fit_transform(train_X)\n",
    "    b = ct.transform(validation_X)\n",
    "    return({'fit': ct, 'train_X': a, 'validation_X': b})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dimensionality_reduction(train_X, validation_X\n",
    "                             , n_comp = None):\n",
    "    if n_comp == None:\n",
    "        return({\"train_X\": train_X, \"validation_X\": validation_X})\n",
    "    else:\n",
    "        c = TruncatedSVD(n_components=n_comp).fit(train_X)\n",
    "        d = c.fit_transform(train_X)\n",
    "        d2 = c.transform(validation_X)\n",
    "        return({'fit': c, 'train_X': d, 'validation_X': d2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interim_results(y, y_pred):\n",
    "    \"\"\"\n",
    "    Assess performance of y_hat vs y\n",
    "\n",
    "    :param y: array of actual labels\n",
    "    :param y_pred: array of predicted labels\n",
    "    :return z: pandas DataFrame, specifying precision, recall and f1 score\n",
    "    \"\"\"\n",
    "    z = pd.DataFrame({'class': y.sort_values().unique()\n",
    "                      ,'precision': precision_recall_fscore_support(y, y_pred, warn_for = ())[0]\n",
    "                      ,'recall': precision_recall_fscore_support(y, y_pred, warn_for = ())[1]\n",
    "                      ,'f1_score': precision_recall_fscore_support(y, y_pred, warn_for = ())[2]                      \n",
    "                     })\n",
    "    return(z)    \n",
    "\n",
    "def classification(train_X, train_y, validation_X, validation_y\n",
    "                   , classifier = [LinearSVC(class_weight = 'balanced')]):\n",
    "    e = classifier.fit(X = train_X, y = train_y)\n",
    "    y_hat = e.predict(X = validation_X)\n",
    "    results = interim_results(validation_y, y_hat)\n",
    "    mean_f1_score = np.mean(results['f1_score'])\n",
    "    return({'fit': e, 'prediction': y_hat, 'results': results, 'mean_f1_score': mean_f1_score})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val(X, y, n_splits, shuffle = False):\n",
    "    splits = dict()\n",
    "    counter = 0\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    for train_index, cv_index in skf.split(X = X, y = y):\n",
    "        train_X = X.iloc[train_index,].copy()\n",
    "        train_y = y.iloc[train_index,].copy()\n",
    "        validation_X = X.iloc[cv_index,].copy()\n",
    "        validation_y = y.iloc[cv_index,].copy()\n",
    "        splits[counter] = {'train_X': train_X, 'train_y': train_y, \n",
    "                           'validation_X': validation_X, 'validation_y': validation_y}\n",
    "        counter += 1\n",
    "    return(splits)\n",
    "\n",
    "def custom_text_class_pipeline(X\n",
    "                               , y\n",
    "                               , cv_splits=2\n",
    "                               , ngram = [(1,2)]\n",
    "                               , min_df = [1], max_df = [1.0]\n",
    "                               , norm = ['l2']\n",
    "                               , use_idf = [True]\n",
    "                               , dim_reduc = [50]\n",
    "                               , classifiers = [LinearSVC(class_weight = 'balanced')]):\n",
    "    \"\"\"\n",
    "    Run pipeline\n",
    "\n",
    "    :param X: array of strings to train on\n",
    "    :param y: array of correct labels\n",
    "    :param cv_splits: number of cross-validation splits to run\n",
    "    :param ngram_range: tuple of how many words to include in each tokenisation (e.g. (1,1) is unigram, (1,2) is bigram etc). See sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "    :param min_df: minimum # documents a word must be found in to include the word in the dictionary (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "    :param max_df: maximum # documents a word must be found in to include the word in the dictionary (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "    :param norm: array of rx_ids\n",
    "    :param use_idf: boolean, whether to use inverse-document-frequency in addition to just text-frequency  \n",
    "    :param smooth_idf: boolean, whether to add one to denominator in idf step to prevent div/0 errors (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "    :param sublinear_tf: boolean, whether to also log-transform the text-frequency step (see sklearn.feature_extraction.text.CountVectorizer for more details) \n",
    "    \"\"\"\n",
    "    params = {'ngram': ngram, 'min_df': min_df, 'max_df': max_df, 'norm': norm, 'use_idf': use_idf}\n",
    "    params = pd.DataFrame(list(itertools.product(*params.values())), columns = params.keys())\n",
    "    \n",
    "    cross_validation = cross_val(X, y, n_splits=cv_splits)\n",
    "    \n",
    "    pbar = tqdm(total=params.shape[0]*len(dim_reduc)*len(classifiers)*cv_splits)\n",
    "    \n",
    "    all_results = pd.DataFrame()\n",
    "    # all_results = pd.DataFrame({ 'k-fold': [\"\"], 'tf-idf': [\"\"], 'tf-idf parameters':[\"\"], 'dimensionality reduction': [\"\"], 'dim_reduc paramters': [\"\"], 'classifier': \"\"}, )\n",
    "\n",
    "    for k in cross_validation:\n",
    "        for row in range(params.shape[0]):\n",
    "            X = tokenisation(cross_validation[k]['train_X']\n",
    "                             , cross_validation[k]['validation_X'] \n",
    "                             , ngram_range = params['ngram'][row]\n",
    "                             , min_df = params['min_df'][row]\n",
    "                             , max_df = params['max_df'][row]\n",
    "                             , norm = params['norm'][row]\n",
    "                             , use_idf = params['use_idf'][row])\n",
    "            for comp in dim_reduc:\n",
    "                X2 = dimensionality_reduction(X['train_X'], X['validation_X']\n",
    "                                              , n_comp = comp)\n",
    "                for j in classifiers:\n",
    "                    classifier_result = classification(X2['train_X']\n",
    "                                                       , cross_validation[k]['train_y']\n",
    "                                                       , X2['validation_X']\n",
    "                                                       , cross_validation[k]['validation_y']\n",
    "                                                       , classifier = j)\n",
    "                    everything = {'k-fold': k\n",
    "                                  , 'tf-idf': copy.deepcopy(X['fit'])\n",
    "                                  , 'tf-idf parameters': params.loc[row,:]\n",
    "                                  , 'dimensionality reduction': copy.deepcopy(X2['fit'])\n",
    "                                  , 'dim_reduc paramters': comp\n",
    "                                  , 'classifier': copy.deepcopy(classifier_result['fit'])\n",
    "                                  , 'results': classifier_result['results']\n",
    "                                  , 'validation_y_hat': classifier_result['prediction']\n",
    "                                  , 'train_X': cross_validation[k]['train_X'] \n",
    "                                  , 'validation_X': cross_validation[k]['validation_X'] \n",
    "                                  , 'train_y': cross_validation[k]['train_y'] \n",
    "                                  , 'validation_y': cross_validation[k]['validation_y']                                   \n",
    "                                  , 'mean_f1_score': np.mean(classifier_result['mean_f1_score'])}\n",
    "                    all_results = all_results.append(everything, ignore_index = True)\n",
    "                    pbar.update()\n",
    "    pbar.close()\n",
    "    return(all_results)\n",
    "\n",
    "def extrapolate(X, results_row):\n",
    "    return(extrapolation.classifier.predict(\n",
    "           extrapolation['dimensionality reduction'].transform(\n",
    "           extrapolation['tf-idf'].transform(all_data.translated)\n",
    "           )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First pass with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_clipboard(sep = '\\t')\n",
    "df.fillna('', inplace=True)\n",
    "df.columns = [re.sub('[^a-zA-Z0-9]+', '_', x.lower()) for x in df.columns]\n",
    "df2 = df.loc[df.body != '',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    64\n",
       "True     35\n",
       "Name: gene_therapy_, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.gene_therapy_.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:11<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "results = custom_text_class_pipeline(X = df2.body.map(str)\n",
    "                                     , y = df2.gene_therapy_\n",
    "                                     , cv_splits = 5\n",
    "                                     , dim_reduc = [50, 100, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.523810\n",
       "1     0.561129\n",
       "2     0.561129\n",
       "3     0.375000\n",
       "4     0.375000\n",
       "5     0.375000\n",
       "6     0.641577\n",
       "7     0.641577\n",
       "8     0.641577\n",
       "9     0.943020\n",
       "10    0.943020\n",
       "11    0.943020\n",
       "12    0.840336\n",
       "13    0.834783\n",
       "14    0.834783\n",
       "Name: mean_f1_score, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mean_f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  f1_score  precision    recall\n",
       "0  False  0.962963   0.928571  1.000000\n",
       "1   True  0.923077   1.000000  0.857143"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.loc[9,'results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([pd.Series(results.loc[9, 'validation_X']).reset_index(drop = True)\n",
    "           , pd.Series(results.loc[9, 'validation_y']).reset_index(drop = True)\n",
    "           , pd.Series(results.loc[9, 'validation_y_hat']).reset_index(drop = True)]\n",
    "          , axis=1).to_csv('results_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extrapolate(X, results_row):\n",
    "    return(extrapolation.classifier.predict(\n",
    "           extrapolation['dimensionality reduction'].transform(\n",
    "           extrapolation['tf-idf'].transform(all_data.translated)\n",
    "           )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extrapolate(df.ab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_health_nlp",
   "language": "python",
   "name": "venv_health_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
