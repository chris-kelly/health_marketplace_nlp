{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Google Sheet Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT PACKAGES\n",
    "\n",
    "# help(\"modules\") \n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET ENVIRONMENT VARIABLES\n",
    "\n",
    "# Credentials\n",
    "CREDENTIALS = \"credentials/credentials.json\"\n",
    "\n",
    "# API data\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "API_SERVICE_NAME = 'sheets'\n",
    "API_VERSION = 'v4'\n",
    "\n",
    "# Google sheet data\n",
    "SPREADSHEET_ID = '158iHeTBUQcVb3spEFajDxyzNV5Bz5O5Oqt0lvuCLGA8'\n",
    "READ_RANGE_NAME = 'cleandata!A1:N'\n",
    "WRITE_RANGE_NAME = 'parseddata!A2:Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET AUTHENTICATION FUNCTION\n",
    "\n",
    "def get_authenticated_service(secret_file = CREDENTIALS\n",
    "                              , scopes = SCOPES\n",
    "                              , api_service_name = API_SERVICE_NAME\n",
    "                              , api_version = API_VERSION):\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(secret_file, scopes)\n",
    "    credentials = flow.run_console()\n",
    "    return build(api_service_name, api_version, credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=941605798195-1aa5774dsksops5hkpd6scvmrsb2pveu.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fspreadsheets&state=fWalDK8XQz31pbCC14A024QJG877DE&prompt=consent&access_type=offline&code_challenge=wKouLqXWylnQ7gysnuplbxweTzVS549KMV7q80WnLCA&code_challenge_method=S256\n",
      "Enter the authorization code: 4/qQGzy5sZCUg0yNDYdY7Cxqg6_BWcA4gnnsu71nnfnGFU0eCI3S8IQ4o\n"
     ]
    }
   ],
   "source": [
    "service = get_authenticated_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT DATA\n",
    "result = service.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID\n",
    "                                             , range=READ_RANGE_NAME).execute()\n",
    "values = result.get('values', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(values)\n",
    "df.columns = values[0]\n",
    "df = df.iloc[1:]\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# String cleaning:\n",
    "########################################\n",
    "import unidecode, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## Clean text strings function\n",
    "########################################\n",
    "\n",
    "def remove_short_strings(X, max_characters = 2, lower_case_only = True):\n",
    "    \"\"\"\n",
    "    Remove 1-2 letter words in list\n",
    "    :param X: List of raw strings\n",
    "    :param max_characters: Maximum size of string to remove\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"    \n",
    "    if lower_case_only:\n",
    "        regex_string = r'\\b[a-z]{1,%s}\\b' % (max_characters)    \n",
    "    else:\n",
    "        regex_string = r'\\b\\w{1,%s}\\b' % (max_characters)\n",
    "    X = list(map(lambda x: re.sub(regex_string,' ', x), X)) # remove 1-2 letter words \n",
    "    return(X)\n",
    "\n",
    "def to_lower(X):\n",
    "    \"\"\"\n",
    "    Set all letters to lowercase\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.lower(), X))\n",
    "    return(X)\n",
    "\n",
    "def to_latin(X):\n",
    "    \"\"\"\n",
    "    Remove non-European characters whilst keeping accented european characters in list\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.encode(\"latin1\", errors=\"ignore\").decode('latin1'), X))\n",
    "    return(X)\n",
    "\n",
    "def replace_accents(X):\n",
    "    \"\"\"\n",
    "    Replace accented characters with non-accented characters in list\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: unidecode.unidecode(x), X))\n",
    "    return(X)\n",
    "\n",
    "def remove_punctuation_regex(remove_tildas = True, remove_numerics = False):\n",
    "    \"\"\"\n",
    "    Write regex to replace all non-alphanumeric characters, replacing them with a space.\n",
    "    Option as to whether to remove tildas (~) or numerical values not.\n",
    "    :param remove_tildas: Boolean, whether to remove tildas or not\n",
    "    :param remove_numerics: Boolean, whether to remove numerics or not    \n",
    "    :return X: Regex to do this\n",
    "    \"\"\"    \n",
    "    if remove_tildas and remove_numerics:\n",
    "        regex_string = r'[^a-zA-Z\\s]' # replace all non-alphabet characters with a space\n",
    "    elif remove_tildas:\n",
    "        regex_string = r'[^\\w\\s]' # replace all non-alphanumeric characters with a space\n",
    "    elif remove_numerics:\n",
    "        regex_string = r'[^a-zA-Z\\s\\~]' # replace all non-alphabet characters except tildas with a space        \n",
    "    else:\n",
    "        regex_string = r'[^\\w\\s\\~]' # replace all non-alphanumeric characters except tildas with a space\n",
    "    return(regex_string)\n",
    "\n",
    "def remove_punctuation(X, regex_string):\n",
    "    \"\"\"\n",
    "    Replace all non-alphanumeric characters in a list of strings, replacing them with a space. \n",
    "    Option as to whether to remove tildas (~) or not.\n",
    "    :param X: List of raw strings\n",
    "    :param remove_tildas: Boolean, whether to remove tildas or not\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"    \n",
    "    X = list(map(lambda x: re.sub(regex_string,' ', x), X)) \n",
    "    return(X)\n",
    "\n",
    "def tokenise(X, delimeter = None):\n",
    "    \"\"\"\n",
    "    Returns list of lists of strings split by the delimeter\n",
    "    :param X: List of strings\n",
    "    :param delimeter: Delimeter to split by\n",
    "    :return X: List of list of strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.split(sep = delimeter), X))\n",
    "    return(X)\n",
    "\n",
    "def remove_stopwords(X, stop_words = stopwords.words('english')):\n",
    "    \"\"\"\n",
    "    Returns list of lists of strings split by the delimeter\n",
    "    :param X: List of strings\n",
    "    :param delimeter: Delimeter to split by\n",
    "    :return X: List of list of strings\n",
    "    \"\"\"\n",
    "    # X = list(map(lambda x: [w if w not in stop_words else None for w in x], X))\n",
    "    X = list(map(lambda x: [w for w in x if w not in stop_words ], X))\n",
    "    return(X)\n",
    "    # https://stackoverflow.com/questions/4260280/if-else-in-a-list-comprehension\n",
    "\n",
    "def stem_strings(X):\n",
    "    \"\"\"\n",
    "    Stems words (shorten algorithmically) them (as defined by SnowballStemmer)\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: SnowballStemmer(\"english\", ignore_stopwords=False).stem(x), X))\n",
    "    return(X)  \n",
    "\n",
    "def lemmatize_strings(X, pos = \"v\"):\n",
    "    \"\"\"\n",
    "    Lemmatize list of strings (as defined by WordNetLemmatizer)\n",
    "    :param X: List of raw strings\n",
    "    :param pos: Pos parameter to feed into WordNetLemmatizer().lemmatize function\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: WordNetLemmatizer().lemmatize(x, pos=pos), X))\n",
    "    return(X)  \n",
    "\n",
    "def clean_strings(X\n",
    "                  , remove_short_str_max_char = 2\n",
    "                  , to_lower_str = True\n",
    "                  , to_latin_str = True\n",
    "                  , replace_accents_str = True\n",
    "                  , regex_string =  r'[^a-zA-Z\\s-]' # remove_punctuation_regex(True, False)\n",
    "                  , tokenise_delimeter = None\n",
    "                  , stop_words = ''\n",
    "                  , stemming_str = False\n",
    "                  , lemma_str = False\n",
    "                  , lemma_pos = \"v\"\n",
    "                  , verbose = False):\n",
    "    \"\"\"\n",
    "    Combination of functions for a list of strings: see parameters\n",
    "    - Replaces non-alpha-numeric characters with whitespace\n",
    "    - Remove english stopwords from and strings and stems them (as defined by SnowballStemmer)\n",
    "    - Lemmatizes english strings (as defined by WordNetLemmatizer) \n",
    "    :param X: List of strings\n",
    "    :param remove_short_str: Numeric, size of small words to remove (if set to 0, no words are removed)\n",
    "    :param to_latin_str: Boolean, whether to remove non-European characters whilst keeping accented european characters from pandas column\n",
    "    :param replace_accents_str: Boolean, whether to replace accented characters with non-accented characters\n",
    "    :param regex_string: String, can add extra regex to find other characters to remove\n",
    "    :param tokenise_delimeter: String, determines how to split into tokens. Default = None splits by all whitespace\n",
    "    :param stop_words: List of stopwords to remove from the tokens\n",
    "    :param stemming_str: Boolean, whether to stem the words or not (do not use before translating) (as defined by SnowballStemmer)\n",
    "    :param lemma_str: Boolean, whether to lemmatize the words or not (do not use before translating) (as defined by WordNetLemmatizer)\n",
    "    :param lemma_pos: String, pos parameter to feed into WordNetLemmatizer().lemmatize function\n",
    "    :param verbose: whether to print when it finishes/comments\n",
    "    :return X: Dataframe of labelled data\n",
    "    \"\"\"\n",
    "    if remove_short_str_max_char > 0:\n",
    "        X = remove_short_strings(X, remove_short_str_max_char) # remove 1-2 letter words     \n",
    "    if to_lower_str: # remove chinese characters, keep accented european characters\n",
    "        X = to_lower(X)     \n",
    "    if to_latin_str: # remove chinese characters, keep accented european characters\n",
    "        X = to_latin(X) \n",
    "    if replace_accents_str: # replace accented characters with non-accented characters\n",
    "        X = replace_accents(X) \n",
    "    X = remove_punctuation(X, regex_string)\n",
    "    X = tokenise(X, tokenise_delimeter)\n",
    "    X = remove_stopwords(X, stop_words)\n",
    "    if stemming_str:\n",
    "        X = list(map(stem_strings, X)) # remove English stopwords from string (as defined by SnowballStemmer)\n",
    "    if lemma_str:\n",
    "        X = list(map(lambda x: lemmatize_strings(x, lemma_pos), X)) # remove English stopwords from string (as defined by SnowballStemmer)        \n",
    "    if verbose:\n",
    "        print(time.strftime('%d/%m/%Y %H:%M:%S') + ' Abstract strings cleaned')\n",
    "    return(X)\n",
    "\n",
    "# https://chrisalbon.com/machine_learning/preprocessing_text/remove_stop_words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/2019 12:16:45 Abstract strings cleaned\n"
     ]
    }
   ],
   "source": [
    "df['clean'] = clean_strings(df.loc[:, 'abstract']\n",
    "                            , regex_string='[^a-zA-Z\\\\\\\\s]'\n",
    "                            , stop_words = stopwords.words('english')\n",
    "                            , verbose = True)\n",
    "df['stem'] = list(map(stem_strings, df['clean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove abstracts with only one word (or less) in\n",
    "df = df.loc[list(map(lambda x: x > 2, list(map(len, df['stem'])))),:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WRITE DATA BACK\n",
    "# df2 = df\n",
    "# df2['clean'] = list(map(lambda x: ' '.join(x), df2['clean']))\n",
    "# df2['stem'] = list(map(lambda x: ' '.join(x), df2['stem']))\n",
    "\n",
    "# service.spreadsheets().values().update(spreadsheetId=SPREADSHEET_ID\n",
    "#                                        , range = WRITE_RANGE_NAME\n",
    "#                                        , valueInputOption = 'RAW'\n",
    "#                                        , body={'values': df2.values.tolist()}).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument as td\n",
    "from gensim.models import Doc2Vec as d2v, FastText as ft #, Word2Vec as w2v, phrases as bigram\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD, NMF # PCA will not work on sparse matricies\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class d2v_structure(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\"Create D2V vectorized features from text\"\"\"\n",
    "    \n",
    "#     def __init__(self, X):\n",
    "#         self.X = X\n",
    "    \n",
    "    def fit(self, X, *_, **args):\n",
    "        tagged_docs = list(map(lambda i, line: td(line, [i])\n",
    "                       , list(range(len(X)))\n",
    "                       , X))\n",
    "        self.d2v_dm = d2v(tagged_docs, **args) # vector_size=100, window=5, min_count=1, workers=4, dm=1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        d2v_dm_m = [self.d2v_dm.infer_vector(x) for x in X]\n",
    "        return(csr_matrix(d2v_dm_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidf_tsvd_structure(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\"Create TF_IDF vectorized features from text\"\"\"\n",
    "    \n",
    "    def fit(self, X, n_components = None, *_, **args):\n",
    "        self.tf_idf = TfidfVectorizer(**args).fit(X) # norm='l2', ngram_range=(1, 2)\n",
    "        self.n_components = n_components\n",
    "        if (n_components != None):\n",
    "            self.t_m = TruncatedSVD(n_components).fit(self.tf_idf.transform(X))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        if (self.n_components != None):\n",
    "            t_m = csr_matrix(self.t_m.transform(self.tf_idf.transform(X)))\n",
    "        else:\n",
    "            t_m = csr_matrix(self.tf_idf.transform(X))\n",
    "        return(t_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df['stem'])\n",
    "\n",
    "d2v_f1 = d2v_structure().fit(X = X, vector_size=150, window=5)\n",
    "d2v_f2 = d2v_structure().fit(X = X, vector_size=150, window=10)\n",
    "d2v_t1 = d2v_f1.transform(X = X)\n",
    "d2v_t2 = d2v_f2.transform(X = X)\n",
    "\n",
    "tfidf_f = tfidf_tsvd_structure().fit(X, n_components = 200, preprocessor = ' '.join)\n",
    "tfidf_t = tfidf_f.transform(X = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434, 500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = hstack([d2v_t1, d2v_t2,tfidf_t]).toarray()\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(train_X, train_y, validation_X, validation_y\n",
    "                   , classifier = [LinearSVC(class_weight = 'balanced')]):\n",
    "    e = classifier.fit(X = train_X, y = train_y)\n",
    "    y_hat = e.predict(X = validation_X)\n",
    "    results = interim_results(validation_y, y_hat)\n",
    "    mean_f1_score = np.mean(results['f1_score'])\n",
    "    return({'fit': e, 'prediction': y_hat, 'results': results, 'mean_f1_score': mean_f1_score})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X, y, n_splits):\n",
    "    splits = dict()\n",
    "    counter = 0\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    for train_index, cv_index in skf.split(X = X, y = y):\n",
    "        train_X = X[train_index].copy()\n",
    "        train_y = y[train_index].copy()\n",
    "        validation_X = X[cv_index].copy()\n",
    "        validation_y = y[cv_index].copy()\n",
    "        splits[counter] = {'train_X': train_X, 'train_y': train_y, \n",
    "                           'validation_X': validation_X, 'validation_y': validation_y}\n",
    "        counter += 1\n",
    "    return(splits)\n",
    "\n",
    "def interim_results(y, y_pred):\n",
    "    \"\"\"\n",
    "    Assess performance of y_hat vs y\n",
    "\n",
    "    :param y: array of actual labels\n",
    "    :param y_pred: array of predicted labels\n",
    "    :return z: pandas DataFrame, specifying precision, recall and f1 score\n",
    "    \"\"\"\n",
    "    z = pd.DataFrame({'class': np.unique(y)\n",
    "                      ,'precision': precision_recall_fscore_support(y, y_pred, warn_for = ())[0]\n",
    "                      ,'recall': precision_recall_fscore_support(y, y_pred, warn_for = ())[1]\n",
    "                      ,'f1_score': precision_recall_fscore_support(y, y_pred, warn_for = ())[2]                      \n",
    "                     })\n",
    "    return(z)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cross_val(X = training_data\n",
    "              , y = np.array((df['neuro'] == 'Y')*1)\n",
    "              , n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classification(train_X = a[0]['train_X']\n",
    "                         , train_y = a[0]['train_y']\n",
    "                         , validation_X = a[0]['validation_X']\n",
    "                         , validation_y = a[0]['validation_y']\n",
    "                         , classifier = LinearSVC(class_weight = 'balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.595745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  precision    recall  f1_score\n",
       "0      0   0.736842  0.500000  0.595745\n",
       "1      1   0.730769  0.883721  0.800000"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_pass(train_X, train_y, validation_X, validation_y, **kwargs):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_health_nlp",
   "language": "python",
   "name": "venv_health_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
