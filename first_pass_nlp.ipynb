{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# General:\n",
    "########################################\n",
    "\n",
    "import pandas as pd, numpy as np, os, sys, copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################\n",
    "# String cleaning:\n",
    "########################################\n",
    "import unidecode, re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "########################################\n",
    "# For the custom pipeline architecture:\n",
    "########################################\n",
    "import itertools\n",
    "# from itertools import chain, itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # to define class for use in pipeline\n",
    "\n",
    "##########\n",
    "# Tokenisation\n",
    "##########\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "\n",
    "##########\n",
    "# Dimensionality reduction\n",
    "##########\n",
    "from sklearn.decomposition import TruncatedSVD, NMF # PCA will not work on sparse matricies\n",
    "\n",
    "##########\n",
    "# Classification\n",
    "##########\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "##########\n",
    "# Pipeline\n",
    "##########\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "##########\n",
    "# Measuring performance\n",
    "##########\n",
    "from sklearn.metrics import average_precision_score, make_scorer, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## Clean text strings function\n",
    "########################################\n",
    "\n",
    "def to_latin(X):\n",
    "    \"\"\"\n",
    "    Remove non-European characters whilst keeping accented european characters from pandas column\n",
    "    :param X: Pandas column of raw strings\n",
    "    :return X: Pandas column of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.encode(\"latin1\", errors=\"ignore\").decode('latin1'), X))\n",
    "    return(X)\n",
    "\n",
    "def replace_accents(X):\n",
    "    \"\"\"\n",
    "    Replace accented characters with non-accented characters\n",
    "    :param X: Pandas column of raw strings\n",
    "    :return X: Pandas column of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: unidecode.unidecode(x), X))\n",
    "    return(X)\n",
    "\n",
    "def remove_short_strings(X):\n",
    "    \"\"\"\n",
    "    Remove 1-2 letter words \n",
    "    :param X: Pandas column of raw strings\n",
    "    :return X: Pandas column of cleaned strings\n",
    "    \"\"\"    \n",
    "    X = list(map(lambda x: re.sub(r'\\b\\w{1,2}\\b',' ', x), X)) # remove 1-2 letter words \n",
    "    return(X)\n",
    "\n",
    "def remove_punctuation(X, remove_tildas = True, remove_numerics = False):\n",
    "    \"\"\"\n",
    "    Replace all non-alphanumeric characters from a string, replacing them with a space. \n",
    "    Option as to whether to remove tildas (~) or not.\n",
    "    :param X: Pandas column of raw strings\n",
    "    :param remove_tildas: Boolean, whether to remove tildas or not\n",
    "    :return X: Pandas column of cleaned strings\n",
    "    \"\"\"    \n",
    "    if remove_tildas and remove_numerics:\n",
    "        X = list(map(lambda x: re.sub(r'[^a-zA-Z\\s]',' ', x), X)) # replace all non-alphanumeric characters with a space\n",
    "    elif remove_tildas:\n",
    "        X = list(map(lambda x: re.sub(r'[^\\w\\s]',' ', x), X)) # replace all non-alphanumeric characters with a space\n",
    "    elif remove_numerics:\n",
    "        X = list(map(lambda x: re.sub(r'[^a-zA-Z\\s\\~]',' ', x), X)) # replace all non-alphabet characters except tildas with a space        \n",
    "    else:\n",
    "        X = list(map(lambda x: re.sub(r'[^\\w\\s\\~]',' ', x), X)) # replace all non-alphanumeric characters except tildas with a space\n",
    "    return(X)\n",
    "\n",
    "def stem_strings(X):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from string (as defined by SnowballStemmer)\n",
    "    :param X: Pandas column of raw strings\n",
    "    :return X: Pandas column of cleaned strings\n",
    "    \"\"\"    \n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "    X = [\\\n",
    "        [' '.join([stemmer.stem(word) for word in x.split()])][0]\\\n",
    "         for x in np.array(X).astype(str)\n",
    "        ]\n",
    "    return(X)   \n",
    "\n",
    "\n",
    "def clean_strings(X\n",
    "                  , to_latin_str = True\n",
    "                  , replace_accents_str = True\n",
    "                  , remove_short_strings_str = True\n",
    "                  , remove_tildas_str = True\n",
    "                  , stemming_str = True\n",
    "                  , verbose = False):\n",
    "    \"\"\"\n",
    "    Combination of functions for a list of strings: \n",
    "    - Removes non-European characters\n",
    "    - Replaces accented characters with non-accented characters\n",
    "    - Removes 1-2 letter words\n",
    "    - Replaces non-alpha-numeric characters with whitespace\n",
    "    - Removes English stopwords from string (as defined by SnowballStemmer)\n",
    "    :param X: Pandas column of strings\n",
    "    :param remove_tildas: Boolean, whether to remove tildas or not\n",
    "    :param stemming: Boolean, whether to stem the words or not (do not use before translating)\n",
    "    :param verbose: whether to print when it finishes/comments\n",
    "    :return X: Dataframe of labelled data\n",
    "    \"\"\"\n",
    "    if to_latin_str: # remove chinese characters, keep accented european characters\n",
    "        X = to_latin(X) \n",
    "    if replace_accents_str: # replace accented characters with non-accented characters\n",
    "        X = replace_accents(X) \n",
    "    if remove_short_strings_str:\n",
    "        X = remove_short_strings(X) # remove 1-2 letter words \n",
    "    if remove_tildas_str:\n",
    "        X = remove_punctuation(X, remove_tildas = remove_tildas_str) # remove non-alpha-numeric characters (potentially except tildas)\n",
    "    if stemming_str:\n",
    "        X = stem_strings(X) # remove English stopwords from string (as defined by SnowballStemmer)\n",
    "    if verbose:\n",
    "    \tprint(time.strftime('%d/%m/%Y %H:%M:%S') + ' Menu item strings cleaned')\n",
    "    return(X)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class custom_tfidf(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self\n",
    "                 , X\n",
    "                 , string_delimeter = ' ~~ '\n",
    "                 , ngram_range= (1,2)\n",
    "                 , min_df = 1\n",
    "                 , max_df = 1.0\n",
    "                 , norm = 'l2'\n",
    "                 , use_idf = True\n",
    "                 , smooth_idf = True\n",
    "                 , sublinear_tf = True):\n",
    "        \"\"\"\n",
    "        Stems and create vocabulary based on delimeter; run text-frequency inverse-document-frequency (tf-idf) \n",
    "\n",
    "        :param X: array of strings\n",
    "        :param string_delimeter: array of strings\n",
    "        :param ngram_range: tuple of how many words to include in each tokenisation (e.g. (1,1) is unigram, (1,2) is bigram etc). See sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "        :param min_df: minimum # documents a word must be found in to include the word in the dictionary (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "        :param max_df: maximum # documents a word must be found in to include the word in the dictionary (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "        :param norm: array of rx_ids\n",
    "        :param use_idf: boolean, whether to use inverse-document-frequency in addition to just text-frequency  \n",
    "        :param smooth_idf: boolean, whether to add one to denominator in idf step to prevent div/0 errors (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "        :param sublinear_tf: boolean, whether to also log-transform the text-frequency step (see sklearn.feature_extraction.text.CountVectorizer for more details) \n",
    "        \"\"\"\n",
    "        self.train_X = X\n",
    "        self.string_delim = string_delimeter\n",
    "        self.ngram = ngram_range\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.norm = norm\n",
    "        self.use_idf = use_idf\n",
    "        self.smooth_idf = smooth_idf\n",
    "        self.sublinear_tf = sublinear_tf\n",
    "        \n",
    "    def fit(self, *_): # kwargs # fit()\n",
    "        \"\"\"\n",
    "        Create vocabulary dictionary (prevents bi+-gramming over seperate documents)\n",
    "        \"\"\"\n",
    "        split_strings = list(itertools.chain.from_iterable([re.split(self.string_delim, x) for x in self.train_X]))\n",
    "        stemmed = clean_strings(split_strings)\n",
    "        count_vec = CountVectorizer(strip_accents = 'unicode'\n",
    "                                    , analyzer = 'word'\n",
    "                                    , stop_words = 'english'\n",
    "                                    , lowercase = True\n",
    "                                    , min_df = self.min_df , max_df = self.max_df\n",
    "                                    , ngram_range = self.ngram)\n",
    "        vocab = count_vec.fit(stemmed)\n",
    "        self.vocab = vocab.vocabulary_\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, X): # kwargs # transform\n",
    "        \"\"\"\n",
    "        Run tf-idf using vocab\n",
    "        \"\"\"        \n",
    "        stemmed = clean_strings(X)\n",
    "        tfidf_vec = TfidfVectorizer(strip_accents = 'unicode'\n",
    "                                    , analyzer = 'word'\n",
    "                                    , stop_words = 'english'\n",
    "                                    , lowercase = True\n",
    "                                    , min_df = self.min_df , max_df = self.max_df\n",
    "                                    , ngram_range = self.ngram\n",
    "                                    , vocabulary = self.vocab\n",
    "                                    , norm = self.norm\n",
    "                                    , use_idf = self.use_idf\n",
    "                                    , smooth_idf =  self.smooth_idf\n",
    "                                    , sublinear_tf = self.sublinear_tf)\n",
    "        vectorized_matrix = tfidf_vec.fit_transform(stemmed)\n",
    "        return(vectorized_matrix)\n",
    "    \n",
    "def tokenisation(train_X, validation_X\n",
    "                 , string_delimeter = ' ~~ '\n",
    "                 , ngram_range= (1,2)\n",
    "                 , min_df = 1\n",
    "                 , max_df = 1.0\n",
    "                 , norm = 'l2'\n",
    "                 , use_idf = True\n",
    "                 , smooth_idf = True\n",
    "                 , sublinear_tf = True):\n",
    "    ct = custom_tfidf(train_X)\n",
    "    a = ct.fit_transform(train_X)\n",
    "    b = ct.transform(validation_X)\n",
    "    return({'fit': ct, 'train_X': a, 'validation_X': b})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dimensionality_reduction(train_X, validation_X\n",
    "                             , n_comp = None):\n",
    "    if n_comp == None:\n",
    "        return({\"train_X\": train_X, \"validation_X\": validation_X})\n",
    "    else:\n",
    "        c = TruncatedSVD(n_components=n_comp).fit(train_X)\n",
    "        d = c.fit_transform(train_X)\n",
    "        d2 = c.transform(validation_X)\n",
    "        return({'fit': c, 'train_X': d, 'validation_X': d2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interim_results(y, y_pred):\n",
    "    \"\"\"\n",
    "    Assess performance of y_hat vs y\n",
    "\n",
    "    :param y: array of actual labels\n",
    "    :param y_pred: array of predicted labels\n",
    "    :return z: pandas DataFrame, specifying precision, recall and f1 score\n",
    "    \"\"\"\n",
    "    z = pd.DataFrame({'class': y.sort_values().unique()\n",
    "                      ,'precision': precision_recall_fscore_support(y, y_pred, warn_for = ())[0]\n",
    "                      ,'recall': precision_recall_fscore_support(y, y_pred, warn_for = ())[1]\n",
    "                      ,'f1_score': precision_recall_fscore_support(y, y_pred, warn_for = ())[2]                      \n",
    "                     })\n",
    "    return(z)    \n",
    "\n",
    "def classification(train_X, train_y, validation_X, validation_y\n",
    "                   , classifier = [LinearSVC(class_weight = 'balanced')]):\n",
    "    e = classifier.fit(X = train_X, y = train_y)\n",
    "    y_hat = e.predict(X = validation_X)\n",
    "    results = interim_results(validation_y, y_hat)\n",
    "    mean_f1_score = np.mean(results['f1_score'])\n",
    "    return({'fit': e, 'prediction': y_hat, 'results': results, 'mean_f1_score': mean_f1_score})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val(X, y, n_splits, shuffle = False):\n",
    "    splits = dict()\n",
    "    counter = 0\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    for train_index, cv_index in skf.split(X = X, y = y):\n",
    "        train_X = X.iloc[train_index,].copy()\n",
    "        train_y = y.iloc[train_index,].copy()\n",
    "        validation_X = X.iloc[cv_index,].copy()\n",
    "        validation_y = y.iloc[cv_index,].copy()\n",
    "        splits[counter] = {'train_X': train_X, 'train_y': train_y, \n",
    "                           'validation_X': validation_X, 'validation_y': validation_y}\n",
    "        counter += 1\n",
    "    return(splits)\n",
    "\n",
    "def custom_text_class_pipeline(X\n",
    "                               , y\n",
    "                               , cv_splits=2\n",
    "                               , ngram = [(1,2)]\n",
    "                               , min_df = [1], max_df = [1.0]\n",
    "                               , norm = ['l2']\n",
    "                               , use_idf = [True]\n",
    "                               , dim_reduc = [50]\n",
    "                               , classifiers = [LinearSVC(class_weight = 'balanced')]):\n",
    "    \"\"\"\n",
    "    Run pipeline\n",
    "\n",
    "    :param X: array of strings to train on\n",
    "    :param y: array of correct labels\n",
    "    :param cv_splits: number of cross-validation splits to run\n",
    "    :param ngram_range: tuple of how many words to include in each tokenisation (e.g. (1,1) is unigram, (1,2) is bigram etc). See sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "    :param min_df: minimum # documents a word must be found in to include the word in the dictionary (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "    :param max_df: maximum # documents a word must be found in to include the word in the dictionary (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "    :param norm: array of rx_ids\n",
    "    :param use_idf: boolean, whether to use inverse-document-frequency in addition to just text-frequency  \n",
    "    :param smooth_idf: boolean, whether to add one to denominator in idf step to prevent div/0 errors (see sklearn.feature_extraction.text.CountVectorizer for more details)\n",
    "    :param sublinear_tf: boolean, whether to also log-transform the text-frequency step (see sklearn.feature_extraction.text.CountVectorizer for more details) \n",
    "    \"\"\"\n",
    "    params = {'ngram': ngram, 'min_df': min_df, 'max_df': max_df, 'norm': norm, 'use_idf': use_idf}\n",
    "    params = pd.DataFrame(list(itertools.product(*params.values())), columns = params.keys())\n",
    "    \n",
    "    cross_validation = cross_val(X, y, n_splits=cv_splits)\n",
    "    \n",
    "    pbar = tqdm(total=params.shape[0]*len(dim_reduc)*len(classifiers)*cv_splits)\n",
    "    \n",
    "    all_results = pd.DataFrame()\n",
    "    # all_results = pd.DataFrame({ 'k-fold': [\"\"], 'tf-idf': [\"\"], 'tf-idf parameters':[\"\"], 'dimensionality reduction': [\"\"], 'dim_reduc paramters': [\"\"], 'classifier': \"\"}, )\n",
    "\n",
    "    for k in cross_validation:\n",
    "        for row in range(params.shape[0]):\n",
    "            X = tokenisation(cross_validation[k]['train_X']\n",
    "                             , cross_validation[k]['validation_X'] \n",
    "                             , ngram_range = params['ngram'][row]\n",
    "                             , min_df = params['min_df'][row]\n",
    "                             , max_df = params['max_df'][row]\n",
    "                             , norm = params['norm'][row]\n",
    "                             , use_idf = params['use_idf'][row])\n",
    "            for comp in dim_reduc:\n",
    "                X2 = dimensionality_reduction(X['train_X'], X['validation_X']\n",
    "                                              , n_comp = comp)\n",
    "                for j in classifiers:\n",
    "                    classifier_result = classification(X2['train_X']\n",
    "                                                       , cross_validation[k]['train_y']\n",
    "                                                       , X2['validation_X']\n",
    "                                                       , cross_validation[k]['validation_y']\n",
    "                                                       , classifier = j)\n",
    "                    everything = {'k-fold': k\n",
    "                                  , 'tf-idf': copy.deepcopy(X['fit'])\n",
    "                                  , 'tf-idf parameters': params.loc[row,:]\n",
    "                                  , 'dimensionality reduction': copy.deepcopy(X2['fit'])\n",
    "                                  , 'dim_reduc paramters': comp\n",
    "                                  , 'classifier': copy.deepcopy(classifier_result['fit'])\n",
    "                                  , 'results': classifier_result['results']\n",
    "                                  , 'validation_y_hat': classifier_result['prediction']\n",
    "                                  , 'train_X': cross_validation[k]['train_X'] \n",
    "                                  , 'validation_X': cross_validation[k]['validation_X'] \n",
    "                                  , 'train_y': cross_validation[k]['train_y'] \n",
    "                                  , 'validation_y': cross_validation[k]['validation_y']                                   \n",
    "                                  , 'mean_f1_score': np.mean(classifier_result['mean_f1_score'])}\n",
    "                    all_results = all_results.append(everything, ignore_index = True)\n",
    "                    pbar.update()\n",
    "    pbar.close()\n",
    "    return(all_results)\n",
    "\n",
    "def extrapolate(X, results_row):\n",
    "    return(extrapolation.classifier.predict(\n",
    "           extrapolation['dimensionality reduction'].transform(\n",
    "           extrapolation['tf-idf'].transform(all_data.translated)\n",
    "           )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First pass with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_clipboard(sep = '\\t')\n",
    "df.fillna('', inplace=True)\n",
    "df.columns = [re.sub('[^a-zA-Z0-9]+', '_', x.lower()) for x in df.columns]\n",
    "df2 = df.loc[df.body != '',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    64\n",
       "True     35\n",
       "Name: gene_therapy_, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.gene_therapy_.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:11<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "results = custom_text_class_pipeline(X = df2.body.map(str)\n",
    "                                     , y = df2.gene_therapy_\n",
    "                                     , cv_splits = 5\n",
    "                                     , dim_reduc = [50, 100, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.523810\n",
       "1     0.561129\n",
       "2     0.561129\n",
       "3     0.375000\n",
       "4     0.375000\n",
       "5     0.375000\n",
       "6     0.641577\n",
       "7     0.641577\n",
       "8     0.641577\n",
       "9     0.943020\n",
       "10    0.943020\n",
       "11    0.943020\n",
       "12    0.840336\n",
       "13    0.834783\n",
       "14    0.834783\n",
       "Name: mean_f1_score, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mean_f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  f1_score  precision    recall\n",
       "0  False  0.962963   0.928571  1.000000\n",
       "1   True  0.923077   1.000000  0.857143"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.loc[9,'results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([pd.Series(results.loc[9, 'validation_X']).reset_index(drop = True)\n",
    "           , pd.Series(results.loc[9, 'validation_y']).reset_index(drop = True)\n",
    "           , pd.Series(results.loc[9, 'validation_y_hat']).reset_index(drop = True)]\n",
    "          , axis=1).to_csv('results_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extrapolate(X, results_row):\n",
    "    return(extrapolation.classifier.predict(\n",
    "           extrapolation['dimensionality reduction'].transform(\n",
    "           extrapolation['tf-idf'].transform(all_data.translated)\n",
    "           )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extrapolate(df.ab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_cuisine_classify)",
   "language": "python",
   "name": "conda_cuisine_classify"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
