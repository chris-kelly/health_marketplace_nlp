{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Google Sheet Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT PACKAGES\n",
    "\n",
    "# help(\"modules\") \n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET ENVIRONMENT VARIABLES\n",
    "\n",
    "# Credentials\n",
    "CREDENTIALS = \"credentials/credentials.json\"\n",
    "\n",
    "# API data\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "API_SERVICE_NAME = 'sheets'\n",
    "API_VERSION = 'v4'\n",
    "\n",
    "# Google sheet data\n",
    "SPREADSHEET_ID = '158iHeTBUQcVb3spEFajDxyzNV5Bz5O5Oqt0lvuCLGA8'\n",
    "READ_RANGE_NAME = 'cleandata!A1:N'\n",
    "WRITE_RANGE_NAME = 'parseddata!A2:Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET AUTHENTICATION FUNCTION\n",
    "\n",
    "def get_authenticated_service(secret_file = CREDENTIALS\n",
    "                              , scopes = SCOPES\n",
    "                              , api_service_name = API_SERVICE_NAME\n",
    "                              , api_version = API_VERSION):\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(secret_file, scopes)\n",
    "    credentials = flow.run_console()\n",
    "    return build(api_service_name, api_version, credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=941605798195-1aa5774dsksops5hkpd6scvmrsb2pveu.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fspreadsheets&state=PMvEV0kujvVdwbtIQL3VBbrHNQQTtd&prompt=consent&access_type=offline&code_challenge=2NU8wHr27eY-utYLDoBnFgwycd3EQBtGHAY8NLIQA4Y&code_challenge_method=S256\n",
      "Enter the authorization code: 4/pAEzUOjNDeGPG13Fn8zWH7zT7OHE1CJGbi9bv5jnXMQs4PcND-1H-QU\n"
     ]
    }
   ],
   "source": [
    "service = get_authenticated_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT DATA\n",
    "result = service.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID\n",
    "                                             , range=READ_RANGE_NAME).execute()\n",
    "values = result.get('values', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(values)\n",
    "df.columns = values[0]\n",
    "df = df.iloc[1:]\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# String cleaning:\n",
    "########################################\n",
    "import unidecode, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## Clean text strings function\n",
    "########################################\n",
    "\n",
    "def remove_short_strings(X, max_characters = 2, lower_case_only = True):\n",
    "    \"\"\"\n",
    "    Remove 1-2 letter words in list\n",
    "    :param X: List of raw strings\n",
    "    :param max_characters: Maximum size of string to remove\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"    \n",
    "    if lower_case_only:\n",
    "        regex_string = r'\\b[a-z]{1,%s}\\b' % (max_characters)    \n",
    "    else:\n",
    "        regex_string = r'\\b\\w{1,%s}\\b' % (max_characters)\n",
    "    X = list(map(lambda x: re.sub(regex_string,' ', x), X)) # remove 1-2 letter words \n",
    "    return(X)\n",
    "\n",
    "def to_lower(X):\n",
    "    \"\"\"\n",
    "    Set all letters to lowercase\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.lower(), X))\n",
    "    return(X)\n",
    "\n",
    "def to_latin(X):\n",
    "    \"\"\"\n",
    "    Remove non-European characters whilst keeping accented european characters in list\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.encode(\"latin1\", errors=\"ignore\").decode('latin1'), X))\n",
    "    return(X)\n",
    "\n",
    "def replace_accents(X):\n",
    "    \"\"\"\n",
    "    Replace accented characters with non-accented characters in list\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: unidecode.unidecode(x), X))\n",
    "    return(X)\n",
    "\n",
    "def remove_punctuation_regex(remove_tildas = True, remove_numerics = False):\n",
    "    \"\"\"\n",
    "    Write regex to replace all non-alphanumeric characters, replacing them with a space.\n",
    "    Option as to whether to remove tildas (~) or numerical values not.\n",
    "    :param remove_tildas: Boolean, whether to remove tildas or not\n",
    "    :param remove_numerics: Boolean, whether to remove numerics or not    \n",
    "    :return X: Regex to do this\n",
    "    \"\"\"    \n",
    "    if remove_tildas and remove_numerics:\n",
    "        regex_string = r'[^a-zA-Z\\s]' # replace all non-alphabet characters with a space\n",
    "    elif remove_tildas:\n",
    "        regex_string = r'[^\\w\\s]' # replace all non-alphanumeric characters with a space\n",
    "    elif remove_numerics:\n",
    "        regex_string = r'[^a-zA-Z\\s\\~]' # replace all non-alphabet characters except tildas with a space        \n",
    "    else:\n",
    "        regex_string = r'[^\\w\\s\\~]' # replace all non-alphanumeric characters except tildas with a space\n",
    "    return(regex_string)\n",
    "\n",
    "def remove_punctuation(X, regex_string):\n",
    "    \"\"\"\n",
    "    Replace all non-alphanumeric characters in a list of strings, replacing them with a space. \n",
    "    Option as to whether to remove tildas (~) or not.\n",
    "    :param X: List of raw strings\n",
    "    :param remove_tildas: Boolean, whether to remove tildas or not\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"    \n",
    "    X = list(map(lambda x: re.sub(regex_string,' ', x), X)) \n",
    "    return(X)\n",
    "\n",
    "def tokenise(X, delimeter = None):\n",
    "    \"\"\"\n",
    "    Returns list of lists of strings split by the delimeter\n",
    "    :param X: List of strings\n",
    "    :param delimeter: Delimeter to split by\n",
    "    :return X: List of list of strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: x.split(sep = delimeter), X))\n",
    "    return(X)\n",
    "\n",
    "def remove_stopwords(X, stop_words = stopwords.words('english')):\n",
    "    \"\"\"\n",
    "    Returns list of lists of strings split by the delimeter\n",
    "    :param X: List of strings\n",
    "    :param delimeter: Delimeter to split by\n",
    "    :return X: List of list of strings\n",
    "    \"\"\"\n",
    "    # X = list(map(lambda x: [w if w not in stop_words else None for w in x], X))\n",
    "    X = list(map(lambda x: [w for w in x if w not in stop_words ], X))\n",
    "    return(X)\n",
    "    # https://stackoverflow.com/questions/4260280/if-else-in-a-list-comprehension\n",
    "\n",
    "def stem_strings(X):\n",
    "    \"\"\"\n",
    "    Stems words (shorten algorithmically) them (as defined by SnowballStemmer)\n",
    "    :param X: List of raw strings\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: SnowballStemmer(\"english\", ignore_stopwords=False).stem(x), X))\n",
    "    return(X)  \n",
    "\n",
    "def lemmatize_strings(X, pos = \"v\"):\n",
    "    \"\"\"\n",
    "    Lemmatize list of strings (as defined by WordNetLemmatizer)\n",
    "    :param X: List of raw strings\n",
    "    :param pos: Pos parameter to feed into WordNetLemmatizer().lemmatize function\n",
    "    :return X: List of cleaned strings\n",
    "    \"\"\"\n",
    "    X = list(map(lambda x: WordNetLemmatizer().lemmatize(x, pos=pos), X))\n",
    "    return(X)  \n",
    "\n",
    "def clean_strings(X\n",
    "                  , remove_short_str_max_char = 2\n",
    "                  , to_lower_str = True\n",
    "                  , to_latin_str = True\n",
    "                  , replace_accents_str = True\n",
    "                  , regex_string =  r'[^a-zA-Z\\s-]' # remove_punctuation_regex(True, False)\n",
    "                  , tokenise_delimeter = None\n",
    "                  , stop_words = ''\n",
    "                  , stemming_str = False\n",
    "                  , lemma_str = False\n",
    "                  , lemma_pos = \"v\"\n",
    "                  , verbose = False):\n",
    "    \"\"\"\n",
    "    Combination of functions for a list of strings: see parameters\n",
    "    - Replaces non-alpha-numeric characters with whitespace\n",
    "    - Remove english stopwords from and strings and stems them (as defined by SnowballStemmer)\n",
    "    - Lemmatizes english strings (as defined by WordNetLemmatizer) \n",
    "    :param X: List of strings\n",
    "    :param remove_short_str: Numeric, size of small words to remove (if set to 0, no words are removed)\n",
    "    :param to_latin_str: Boolean, whether to remove non-European characters whilst keeping accented european characters from pandas column\n",
    "    :param replace_accents_str: Boolean, whether to replace accented characters with non-accented characters\n",
    "    :param regex_string: String, can add extra regex to find other characters to remove\n",
    "    :param tokenise_delimeter: String, determines how to split into tokens. Default = None splits by all whitespace\n",
    "    :param stop_words: List of stopwords to remove from the tokens\n",
    "    :param stemming_str: Boolean, whether to stem the words or not (do not use before translating) (as defined by SnowballStemmer)\n",
    "    :param lemma_str: Boolean, whether to lemmatize the words or not (do not use before translating) (as defined by WordNetLemmatizer)\n",
    "    :param lemma_pos: String, pos parameter to feed into WordNetLemmatizer().lemmatize function\n",
    "    :param verbose: whether to print when it finishes/comments\n",
    "    :return X: Dataframe of labelled data\n",
    "    \"\"\"\n",
    "    if remove_short_str_max_char > 0:\n",
    "        X = remove_short_strings(X, remove_short_str_max_char) # remove 1-2 letter words     \n",
    "    if to_lower_str: # remove chinese characters, keep accented european characters\n",
    "        X = to_lower(X)     \n",
    "    if to_latin_str: # remove chinese characters, keep accented european characters\n",
    "        X = to_latin(X) \n",
    "    if replace_accents_str: # replace accented characters with non-accented characters\n",
    "        X = replace_accents(X) \n",
    "    X = remove_punctuation(X, regex_string)\n",
    "    X = tokenise(X, tokenise_delimeter)\n",
    "    X = remove_stopwords(X, stop_words)\n",
    "    if stemming_str:\n",
    "        X = list(map(stem_strings, X)) # remove English stopwords from string (as defined by SnowballStemmer)\n",
    "    if lemma_str:\n",
    "        X = list(map(lambda x: lemmatize_strings(x, lemma_pos), X)) # remove English stopwords from string (as defined by SnowballStemmer)        \n",
    "    if verbose:\n",
    "        print(time.strftime('%d/%m/%Y %H:%M:%S') + ' Abstract strings cleaned')\n",
    "    return(X)\n",
    "\n",
    "# https://chrisalbon.com/machine_learning/preprocessing_text/remove_stop_words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/08/2019 12:56:07 Abstract strings cleaned\n"
     ]
    }
   ],
   "source": [
    "df['clean'] = clean_strings(df.loc[:, 'abstract']\n",
    "                            , regex_string='[^a-zA-Z\\\\\\\\s]'\n",
    "                            , stop_words = stopwords.words('english')\n",
    "                            , verbose = True)\n",
    "df['stem'] = list(map(stem_strings, df['clean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '158iHeTBUQcVb3spEFajDxyzNV5Bz5O5Oqt0lvuCLGA8',\n",
       " 'updatedRange': 'parseddata!A2:P351',\n",
       " 'updatedRows': 350,\n",
       " 'updatedColumns': 16,\n",
       " 'updatedCells': 5600}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE DATA BACK\n",
    "df2 = df\n",
    "df2['clean'] = list(map(lambda x: ' '.join(x), df2['clean']))\n",
    "df2['stem'] = list(map(lambda x: ' '.join(x), df2['stem']))\n",
    "\n",
    "service.spreadsheets().values().update(spreadsheetId=SPREADSHEET_ID\n",
    "                                       , range = WRITE_RANGE_NAME\n",
    "                                       , valueInputOption = 'RAW'\n",
    "                                       , body={'values': df2.values.tolist()}).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument as td\n",
    "from gensim.models import Doc2Vec as d2v, FastText as ft #, Word2Vec as w2v, phrases as bigram\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD, NMF # PCA will not work on sparse matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_data(X, y, n_splits):\n",
    "    splits = dict()\n",
    "    counter = 0\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    for train_index, cv_index in skf.split(X = X, y = y):\n",
    "        train_X = X[train_index].copy()\n",
    "        train_y = y[train_index].copy()\n",
    "        validation_X = X[cv_index].copy()\n",
    "        validation_y = y[cv_index].copy()\n",
    "        splits[counter] = {'train_X': train_X, 'train_y': train_y, \n",
    "                           'validation_X': validation_X, 'validation_y': validation_y}\n",
    "        counter += 1\n",
    "    return(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = w2v(df['stem'], size=100, window=5, min_count=1, workers=4)\n",
    "# w2v_vectors = w2v_model.wv\n",
    "# w2v_model.wv.similarity('beta', 'tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 100)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_docs = list(map(lambda i, line: td(line, [i])\n",
    "                       , df.index, df.loc[df.index,'stem']))\n",
    "# VECTO RIZE WORDS USING DISTRIBUTED MEMORY DOC-2-VEC\n",
    "d2v_dm = d2v(tagged_docs, vector_size=100, window=5, min_count=1, workers=4, dm=1)\n",
    "d2v_dm_m = [d2v_dm.infer_vector(x) for x in list(df['stem'])]\n",
    "d2v_dm_m = csr_matrix(d2v_dm_m)\n",
    "d2v_dm_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 150)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(preprocessor=' '.join, norm='l2'\n",
    "                         , ngram_range=(1, 2)\n",
    "                         # , token_pattern = '[^ ]+'\n",
    "                         # , token_pattern = '[^a-zA-Z\\\\\\\\s]'\n",
    "                         ) # norm=None # IF WANT ACTUAL WORD COUNTS, RATHER THAN L2\n",
    "tf_idf_m = tf_idf.fit_transform(df['stem'])\n",
    "tf_idf_m.shape\n",
    "# tf_m = tf_idf_m.multiply(1/tf_idf.idf_)\n",
    "t_svd = TruncatedSVD(n_components=150)\n",
    "t_svd_m = t_svd.fit_transform(tf_idf_m)\n",
    "t_svd_m = csr_matrix(t_svd_m)\n",
    "t_svd_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = hstack([d2v_dm_m,t_svd_m]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X, y, n_splits):\n",
    "    splits = dict()\n",
    "    counter = 0\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    for train_index, cv_index in skf.split(X = X, y = y):\n",
    "        train_X = X[train_index].copy()\n",
    "        train_y = y[train_index].copy()\n",
    "        validation_X = X[cv_index].copy()\n",
    "        validation_y = y[cv_index].copy()\n",
    "        splits[counter] = {'train_X': train_X, 'train_y': train_y, \n",
    "                           'validation_X': validation_X, 'validation_y': validation_y}\n",
    "        counter += 1\n",
    "    return(splits)\n",
    "\n",
    "def interim_results(y, y_pred):\n",
    "    \"\"\"\n",
    "    Assess performance of y_hat vs y\n",
    "\n",
    "    :param y: array of actual labels\n",
    "    :param y_pred: array of predicted labels\n",
    "    :return z: pandas DataFrame, specifying precision, recall and f1 score\n",
    "    \"\"\"\n",
    "    z = pd.DataFrame({'class': np.unique(y)\n",
    "                      ,'precision': precision_recall_fscore_support(y, y_pred, warn_for = ())[0]\n",
    "                      ,'recall': precision_recall_fscore_support(y, y_pred, warn_for = ())[1]\n",
    "                      ,'f1_score': precision_recall_fscore_support(y, y_pred, warn_for = ())[2]                      \n",
    "                     })\n",
    "    return(z)    \n",
    "\n",
    "def classification(train_X, train_y, validation_X, validation_y\n",
    "                   , classifier = [LinearSVC(class_weight = 'balanced')]):\n",
    "    e = classifier.fit(X = train_X, y = train_y)\n",
    "    y_hat = e.predict(X = validation_X)\n",
    "    results = interim_results(validation_y, y_hat)\n",
    "    mean_f1_score = np.mean(results['f1_score'])\n",
    "    return({'fit': e, 'prediction': y_hat, 'results': results, 'mean_f1_score': mean_f1_score})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cross_val(X = training_data\n",
    "              , y = np.array((df['neuro'] == 'Y')*1)\n",
    "              , n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classification(train_X = a[0]['train_X']\n",
    "                         , train_y = a[0]['train_y']\n",
    "                         , validation_X = a[0]['validation_X']\n",
    "                         , validation_y = a[0]['validation_y']\n",
    "                         , classifier = LinearSVC(class_weight = 'balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.808511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  precision    recall  f1_score\n",
       "0      0   0.750000  0.535714  0.625000\n",
       "1      1   0.745098  0.883721  0.808511"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_health_nlp",
   "language": "python",
   "name": "venv_health_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
