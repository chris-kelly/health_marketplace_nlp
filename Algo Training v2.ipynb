{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Classification Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>22368089</td>\n",
       "      <td>N</td>\n",
       "      <td>The Cat-301 monoclonal antibody identifies agg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>30549480</td>\n",
       "      <td>N</td>\n",
       "      <td>Objective: To characterize the prevalence of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>30534812</td>\n",
       "      <td>N</td>\n",
       "      <td>We conducted a retrospective study, between 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>30532051</td>\n",
       "      <td>Y</td>\n",
       "      <td>Antipsychotic (AP) drugs are used to treat psy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>30531921</td>\n",
       "      <td>Y</td>\n",
       "      <td>Neural prostheses decode intention from cortic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id label                                           abstract\n",
       "0  22368089     N  The Cat-301 monoclonal antibody identifies agg...\n",
       "1  30549480     N  Objective: To characterize the prevalence of m...\n",
       "2  30534812     N  We conducted a retrospective study, between 20...\n",
       "3  30532051     Y  Antipsychotic (AP) drugs are used to treat psy...\n",
       "4  30531921     Y  Neural prostheses decode intention from cortic..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('csv/tech_no_tech.csv')\n",
    "df.columns = ['id', 'label', 'abstract']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    137\n",
       "Y     77\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import unidecode, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(x\n",
    "                 , regex_string = ['[^\\w\\s]','\\\\n']\n",
    "                 , replacement = ' '):\n",
    "    x = x.lower()\n",
    "    x = x.encode(\"latin1\", errors=\"ignore\").decode('latin1')\n",
    "    x = unidecode.unidecode(x)\n",
    "    for i in regex_string:\n",
    "        x = re.sub(i, replacement, x)\n",
    "    x = re.sub(' +', ' ', x).strip()\n",
    "    return(x)\n",
    "\n",
    "def tokenize(x, delimeter = ' '):\n",
    "    x = x.split(sep = delimeter)\n",
    "    return(x)\n",
    "\n",
    "def clean_tokens(x\n",
    "                 , stop_words = stopwords.words('english')\n",
    "                 , min_string_length = 2\n",
    "                 , stem = True\n",
    "                 , lemmatize_pos = None\n",
    "                 , sort=True):\n",
    "    x = [w for w in x if w not in stop_words]\n",
    "    x = [w for w in x if len(w) >= min_string_length]\n",
    "    if stem:\n",
    "        x = [SnowballStemmer(\"english\", ignore_stopwords=False).stem(w) for w in x]\n",
    "    if lemmatize_pos is not None: # lemmatize_pos = 'v'\n",
    "        x = [WordNetLemmatizer().lemmatize(w, pos=lemmatize_pos) for w in x]\n",
    "    if sort:\n",
    "        x = sorted(x)\n",
    "    return(x)\n",
    "    \n",
    "def clean_all(x\n",
    "              , regex_string = ['[^\\w\\s]','\\\\n']\n",
    "              , replacement = ' '\n",
    "              , delimeter = ' '\n",
    "              , stop_words = stopwords.words('english')\n",
    "              , min_string_length = 2\n",
    "              , stem = True\n",
    "              , lemmatize_pos = None\n",
    "              , sort=False):\n",
    "    x = clean_string(x\n",
    "                     , regex_string = regex_string\n",
    "                     , replacement = replacement)\n",
    "    x = tokenize(x\n",
    "                , delimeter = delimeter)\n",
    "    x = clean_tokens(x\n",
    "                     , stop_words = stop_words\n",
    "                     , min_string_length = min_string_length\n",
    "                     , stem = stem\n",
    "                     , lemmatize_pos = lemmatize_pos\n",
    "                     , sort = sort)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## building class so later we can add to the text pipeline\n",
    "\n",
    "class StringClean(BaseEstimator, TransformerMixin):    \n",
    "    def __init__(self\n",
    "                 , regex_string = ['[^\\w\\s]','\\\\n']\n",
    "                 , replacement = ' '\n",
    "                 , stop_words = stopwords.words('english')\n",
    "                 , min_string_length = 2\n",
    "                 , stem = True\n",
    "                 , lemmatize_pos = None):\n",
    "        self.regex_string = regex_string\n",
    "        self.replacement = replacement  \n",
    "        self.stop_words = stop_words\n",
    "        self.min_string_length = min_string_length\n",
    "        self.stem = stem\n",
    "        self.lemmatize_pos = lemmatize_pos\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return(self)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = [clean_all(x) for x in X]\n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = StringClean().fit_transform(df.abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack # for stacking dimensionality reduction matricies\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument as td\n",
    "from gensim.models import Doc2Vec as d2v #, phrases as bigram # Use sklearn tfidf vectorizer instead, as ngram > 2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD, NMF # PCA will not work on sparse matricies\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose columns to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class col_chooser(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"Choose which heterogeneous feature to feed into the pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, key = ''):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        try:\n",
    "            return(X[self.key])\n",
    "        except:\n",
    "            return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidf_tsvd_struct_pipeline(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\"Create TF_IDF vectorized features from text\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=100, norm='l2', ngram_range=(1, 2), preprocessor = ' '.join):\n",
    "        self.n_components = n_components\n",
    "        self.norm = norm\n",
    "        self.ngram_range = ngram_range\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "    def fit(self, X, *_, **args):\n",
    "        if X is not None:\n",
    "            self.tf_idf = TfidfVectorizer(norm = self.norm\n",
    "                                          , ngram_range = self.ngram_range\n",
    "                                          , preprocessor = self.preprocessor\n",
    "                                          , **args).fit(X) # norm='l2', ngram_range=(1, 2)\n",
    "            # self.n_components = n_components\n",
    "            if (self.n_components != None):\n",
    "                self.t_m = TruncatedSVD(self.n_components).fit(self.tf_idf.transform(X))\n",
    "            return(self)\n",
    "        else:\n",
    "            return(self)\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        if self.tf_idf is not None:\n",
    "            if (self.n_components != None):\n",
    "                t_m = csr_matrix(self.t_m.transform(self.tf_idf.transform(X)))\n",
    "            else:\n",
    "                t_m = csr_matrix(self.tf_idf.transform(X))\n",
    "            return(t_m)\n",
    "        else:\n",
    "            return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class d2v_struct_pipeline(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\"Create D2V vectorized features from text\"\"\"\n",
    "    \n",
    "    # https://arxiv.org/pdf/1405.4053v2.pdf\n",
    "    # https://arxiv.org/pdf/1301.3781.pdf\n",
    "    \n",
    "    # https://medium.com/@amarbudhiraja/understanding-document-embeddings-of-doc2vec-bfe7237a26da\n",
    "    \n",
    "    def __init__(self, vector_size=100, window=10, min_count=1, dm=1): # learning_rate=0.02, epochs=20\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.dm = dm\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.epochs = epochs\n",
    "    \n",
    "    def fit(self, X, *_, **args):\n",
    "        tagged_docs = list(map(lambda i, line: td(line, [i]), list(range(len(X))), X))\n",
    "        self.d2v_dm = d2v(tagged_docs\n",
    "                          , vector_size=self.vector_size\n",
    "                          , window=self.window\n",
    "                          , min_count=self.min_count\n",
    "                          , dm=self.dm\n",
    "                          , **args)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        d2v_dm_m = [self.d2v_dm.infer_vector(x) for x in X]\n",
    "        return(csr_matrix(d2v_dm_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch Pipeline (should use randomized search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PERMIT MULTIPLE UNSUPERVISED EMBEDDING OPTIONS\n",
    "unsupervised_union = \\\n",
    "FeatureUnion([(\"tfidf_svd\", tfidf_tsvd_struct_pipeline(preprocessor = ' '.join))\n",
    "              , (\"d2v1\", d2v_struct_pipeline(dm=1))\n",
    "              , (\"d2v0\", d2v_struct_pipeline(dm=0))])\n",
    "\n",
    "## ALLOW VARYING NUMBER OF TEXT FIELDS\n",
    "text_inputs = \\\n",
    "FeatureUnion([('text_1', Pipeline([('col', col_chooser(key = 'text')) # key = 'abstract'\n",
    "                                   # , ('str_clean', StringClean())\n",
    "                                   , ('comb', unsupervised_union)]))\n",
    "#               ,('text_2', Pipeline([('col', col_chooser(key = 'stem_rx_description'))\n",
    "#                                     , ('comb', unsupervised_union)]))\n",
    "#               ,('text_3', Pipeline([('col', col_chooser())\n",
    "#                                     , ('comb', unsupervised_union)]))\n",
    "#               ,('text_4', Pipeline([('col', col_chooser())\n",
    "#                                     , ('comb', unsupervised_union)]))\n",
    "             ])\n",
    "\n",
    "## CLASSIFICATION\n",
    "classifier = CalibratedClassifierCV(LinearSVC(class_weight = 'balanced')) # SVC(kernel=\"linear\")\n",
    "\n",
    "# PUT IT IN PIPELINE\n",
    "pipeline = Pipeline([(\"features\", text_inputs)\n",
    "                     , (\"classifer\", classifier)]) # memory=cachedir\n",
    "\n",
    "# DEFINE PERFORMANCE METRICS\n",
    "scoring = {'f1_macro': 'f1_macro'\n",
    "           , 'roc_curve': 'roc_auc'\n",
    "           , 'precision': 'precision_macro'\n",
    "           , 'recall': 'recall_macro'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define gridsearch parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# from sagemaker import get_execution_role\n",
    "# from sagemaker.sklearn import SKLearn\n",
    "\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# role = get_execution_role()\n",
    "# instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "# train_data_location = sagemaker_session.upload_data(\n",
    "#     path='../csv/tech_no_tech.csv', key_prefix=\"data\"\n",
    "# )\n",
    "\n",
    "# sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET NAMES OF PARAMETERS FOR GRIDSEARCH\n",
    "# sorted(pipeline.get_params().keys())\n",
    "\n",
    "# DEFINE GRIDSEARCH\n",
    "param_grid = dict(  features__text_1__comb__tfidf_svd__n_components=[400]\n",
    "                  , features__text_1__comb__d2v1__vector_size=[300]\n",
    "                  , features__text_1__comb__d2v0__vector_size=[400]\n",
    "#                   , features__transformer_weights = [{'text_1': 1\n",
    "#                                                       , 'text_2': 1\n",
    "#                                                       , 'text_3': 0\n",
    "#                                                       , 'text_4': 0}]\n",
    "                 )\n",
    "\n",
    "grid_search = GridSearchCV(pipeline\n",
    "                           , param_grid=param_grid\n",
    "                           , cv=5\n",
    "                           , scoring = scoring\n",
    "                           , refit='roc_curve' #, refit = False # \n",
    "                           , return_train_score=True\n",
    "                           , verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(df,df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_fit = grid_search.fit(X = X_train.to_records()\n",
    "                         , y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(gs_fit.cv_results_ )\n",
    "cv_results[['params'] + [x for x in list(cv_results.columns) if 'mean' in x and 'test' in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[[x for x in list(cv_results.columns) if 'split' in x and 'test_roc_curve' in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET BEST PREDICTED CLASS\n",
    "\n",
    "# df['prediction'] = gs_fit.predict(df)\n",
    "\n",
    "X_test['prediction'] = gs_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET PROBABILITIES PER CLASS\n",
    "\n",
    "# df[list(gs_fit.classes_)] = \\\n",
    "# pd.DataFrame(gs_fit.predict_proba(df)).reset_index(drop = True)\n",
    "\n",
    "# X_test[list(gs_fit.classes_)] = gs_fit.predict_proba(X_test)\n",
    "X_test['prob_a'] = [x[0] for x in gs_fit.predict_proba(X_test).tolist()]\n",
    "# pd.DataFrame(\n",
    "#).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X_test.label == X_test.prediction)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('csv/tech_test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_health_nlp",
   "language": "python",
   "name": "venv_health_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
